# app/core.py
import os
import chromadb
from sentence_transformers import SentenceTransformer, CrossEncoder
from openai import OpenAI
#from langchain.text_splitter import RecursiveCharacterTextSplitter
# æ–°ç‰ˆlangchainæ³¨æ„ä¸­é—´æ˜¯ä¸‹åˆ’çº¿ _
from langchain_text_splitters import RecursiveCharacterTextSplitter

# å¼•å…¥æˆ‘ä»¬åˆšæ‰å†™å¥½çš„ Utils å’Œ Config
from app.config import settings
from app.utils.ocr import ocr_engine
from app.utils.bm25 import bm25_retriever
from app.schemas import SourceDocument

class RAGService:
    def __init__(self):
        print("ğŸš€ [Core] æ­£åœ¨åˆå§‹åŒ– RAG æ ¸å¿ƒæœåŠ¡...")
        
        # 1. åˆå§‹åŒ– Chroma
        self.chroma_client = chromadb.PersistentClient(path=settings.DB_PATH)
        self.collection = self.chroma_client.get_or_create_collection(name=settings.DB_NAME)
        
        # 2. åˆå§‹åŒ– Embedder (å‘é‡æ¨¡å‹)
        print(f"   Load Embedding: {settings.MODEL_PATH}")
        self.embed_model = SentenceTransformer(settings.MODEL_PATH, local_files_only=True)
        
        # 3. åˆå§‹åŒ– Reranker (ç²¾æ’æ¨¡å‹)
        print(f"   Load Reranker: {settings.RERANKER_PATH}")
        self.reranker = CrossEncoder(settings.RERANKER_PATH, local_files_only=True)
        
        # 4. åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.llm_client = OpenAI(
            api_key=settings.AI_API_KEY,
            base_url=settings.AI_BASE_URL
        )
        
        # 5. ç¡®ä¿ BM25 å’Œ Chroma åŒæ­¥ (ç³»ç»Ÿå¯åŠ¨æ—¶æ£€æŸ¥)
        # å¦‚æœ BM25 æ˜¯ç©ºçš„ä½† Chroma æœ‰æ•°æ®ï¼Œå°è¯•é‡å»º(æ­¤å¤„ç•¥ï¼Œä¸ºåŠ é€Ÿå¯åŠ¨æš‚ä¸è‡ªåŠ¨å…¨é‡é‡å»º)
        
        print("âœ… [Core] æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    def _rrf_fusion(self, vector_results, bm25_results, k=60):
        """å€’æ•°æ’åèåˆç®—æ³• (RRF)"""
        fused_scores = {}
        
        # å½’ä¸€åŒ–æ•°æ®ç»“æ„
        # vector_results: {'documents': [[...]], 'metadatas': [[...]]}
        # bm25_results: ([docs], [metas])
        
        vec_docs = vector_results['documents'][0]
        vec_metas = vector_results['metadatas'][0]
        
        bm25_docs = bm25_results[0]
        bm25_metas = bm25_results[1]
        
        # å»ºç«‹å†…å®¹åˆ°å…ƒæ•°æ®çš„æ˜ å°„
        content_map = {}
        
        # ç§¯åˆ†
        for rank, doc in enumerate(vec_docs):
            if doc not in fused_scores: fused_scores[doc] = 0
            fused_scores[doc] += 1 / (k + rank + 1)
            content_map[doc] = vec_metas[rank]
            
        for rank, doc in enumerate(bm25_docs):
            if doc not in fused_scores: fused_scores[doc] = 0
            fused_scores[doc] += 1 / (k + rank + 1)
            content_map[doc] = bm25_metas[rank]
            
        # æ’åº
        sorted_docs = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_docs, content_map

    def search(self, query: str, top_k: int = 3) -> tuple[list, list, list]:
        """
        æ··åˆæ£€ç´¢å…¥å£ï¼šVector(20) + BM25(20) -> RRF -> Reranker -> TopK
        """
        # 1. å‘é‡æ£€ç´¢
        query_vec = self.embed_model.encode([query]).tolist()
        vec_res = self.collection.query(query_embeddings=query_vec, n_results=settings.DEFAULT_TOP_K)
        
        # 2. BM25 æ£€ç´¢
        bm25_docs, bm25_metas = bm25_retriever.search(query, top_k=settings.DEFAULT_TOP_K)
        
        # 3. RRF èåˆ
        sorted_candidates, content_map = self._rrf_fusion(vec_res, (bm25_docs, bm25_metas))
        
        # å–å‰ 20 ä¸ªåšç²¾æ’
        candidates = sorted_candidates[:20]
        
        if not candidates:
            return [], [], []

        # 4. Rerank é‡æ’åº
        rerank_inputs = [[query, doc[0]] for doc in candidates]
        scores = self.reranker.predict(rerank_inputs)
        
        # ç»„åˆç»“æœ (Score, Doc, Meta)
        final_results = []
        for i, score in enumerate(scores):
            doc_content = rerank_inputs[i][1]
            final_results.append({
                "content": doc_content,
                "meta": content_map[doc_content],
                "score": float(score)
            })
            
        # æŒ‰ Reranker åˆ†æ•°æ’åºå¹¶æˆªæ–­
        final_results.sort(key=lambda x: x["score"], reverse=True)
        final_results = final_results[:top_k]
        
        return (
            [x["content"] for x in final_results],
            [x["meta"] for x in final_results],
            [x["score"] for x in final_results]
        )

    def chat(self, query: str, history: list, top_k: int = 3):
        """
        å¯¹è¯ä¸»é€»è¾‘ï¼šSearch -> Prompt -> LLM
        """
        # (å¯é€‰) è¿™é‡Œå¯ä»¥åŠ  Query Rewrite é€»è¾‘
        
        # æ‰§è¡Œæœç´¢
        docs, metas, scores = self.search(query, top_k)
        
        # æ„é€  Prompt
        if not docs:
            return {"answer": "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", "docs": [], "metas": [], "scores": []}
            
        context_str = "\n\n".join([f"ç‰‡æ®µ{i+1}: {d}" for i, d in enumerate(docs)])
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åˆ¶é€ é¢†åŸŸçš„ä¸“å®¶åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
        å¦‚æœå‚è€ƒèµ„æ–™ä¸è¶³ä»¥å›ç­”ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ã€‚
        
        ã€å‚è€ƒèµ„æ–™ã€‘
        {context_str}
        """
        
        # è°ƒç”¨ LLM
        response = self.llm_client.chat.completions.create(
            model=settings.LLM_MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ],
            temperature=0.3
        )
        
        return {
            "answer": response.choices[0].message.content,
            "docs": docs,
            "metas": metas,
            "scores": scores
        }

    def process_upload(self, temp_path: str, filename: str, use_ocr: bool):
        """
        æ–‡ä»¶å¤„ç†æµç¨‹ï¼šæå– -> åˆ‡ç‰‡ -> å­˜å‘é‡åº“ -> å­˜BM25
        """
        # 1. æå–
        pages = ocr_engine.extract_text(temp_path, force_ocr=use_ocr)
        if not pages: return 0
        
        # 2. åˆ‡ç‰‡
        # text_splitter = RecursiveCharacterTextSplitter(
        #     chunk_size=settings.CHUNK_SIZE,
        #     chunk_overlap=settings.CHUNK_OVERLAP
        # )
        # 2. åˆ‡ç‰‡ (ä»£ç ä¸å˜ï¼Œä½†åº•å±‚è°ƒç”¨çš„åº“å˜äº†)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""] # æ˜¾å¼æŒ‡å®šä¸­æ–‡åˆ†éš”ç¬¦æ›´ç¨³
        )

        docs_to_add = []
        metas_to_add = []
        ids_to_add = []
        
        for page_num, text in pages:
            chunks = text_splitter.split_text(text)
            for i, chunk in enumerate(chunks):
                docs_to_add.append(chunk)
                metas_to_add.append({
                    "source": filename,
                    "page": page_num
                })
                ids_to_add.append(f"{filename}_p{page_num}_c{i}")
        
        # 3. å­˜å…¥ Chroma (è‡ªåŠ¨è®¡ç®—å‘é‡)
        if docs_to_add:
            # è¿™é‡Œçš„ batch å¤„ç†é€šå¸¸ç”± Chroma å†…éƒ¨å¤„ç†ï¼Œä½†é‡å¤§å»ºè®®åˆ†æ‰¹
            embeddings = self.embed_model.encode(docs_to_add).tolist()
            self.collection.upsert(
                documents=docs_to_add,
                embeddings=embeddings,
                metadatas=metas_to_add,
                ids=ids_to_add
            )
            
            # 4. å­˜å…¥ BM25
            bm25_retriever.add_documents(docs_to_add, metas_to_add)
            
        return len(docs_to_add)

# åˆå§‹åŒ–å…¨å±€å•ä¾‹
rag_service = RAGService()